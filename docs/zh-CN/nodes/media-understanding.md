---
summary: "20. å…¥ç«™å›¾åƒ/éŸ³é¢‘/è§†é¢‘ç†è§£ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒæä¾›å•† + CLI å›é€€"
read_when:
  - 21. è®¾è®¡æˆ–é‡æ„åª’ä½“ç†è§£
  - 22. è°ƒä¼˜å…¥ç«™éŸ³é¢‘/è§†é¢‘/å›¾åƒé¢„å¤„ç†
title: "23. åª’ä½“ç†è§£"
---

# 24. åª’ä½“ç†è§£ï¼ˆå…¥ç«™ï¼‰â€” 2026-01-17

25. OpenClaw å¯åœ¨å›å¤æµæ°´çº¿è¿è¡Œå‰ **æ±‡æ€»å…¥ç«™åª’ä½“**ï¼ˆå›¾åƒ/éŸ³é¢‘/è§†é¢‘ï¼‰ã€‚ 26. å®ƒä¼šè‡ªåŠ¨æ£€æµ‹æœ¬åœ°å·¥å…·æˆ–æä¾›å•†å¯†é’¥æ˜¯å¦å¯ç”¨ï¼Œå¹¶ä¸”å¯ä»¥è¢«ç¦ç”¨æˆ–è‡ªå®šä¹‰ã€‚ 27. å¦‚æœå…³é—­ç†è§£ï¼Œæ¨¡å‹ä»ä¼šåƒå¾€å¸¸ä¸€æ ·æ¥æ”¶åŸå§‹æ–‡ä»¶/URLã€‚

## 28. ç›®æ ‡

- 29. å¯é€‰ï¼šå°†å…¥ç«™åª’ä½“é¢„æ¶ˆåŒ–ä¸ºç®€çŸ­æ–‡æœ¬ï¼Œä»¥å®ç°æ›´å¿«è·¯ç”± + æ›´å¥½çš„å‘½ä»¤è§£æã€‚
- 30. å§‹ç»ˆä¿ç•™å‘æ¨¡å‹äº¤ä»˜åŸå§‹åª’ä½“ï¼ˆå§‹ç»ˆï¼‰ã€‚
- 31. æ”¯æŒ **æä¾›å•† API** å’Œ **CLI å›é€€**ã€‚
- 32. å…è®¸å¤šä¸ªæ¨¡å‹å¹¶æŒ‰é¡ºåºå›é€€ï¼ˆé”™è¯¯/å¤§å°/è¶…æ—¶ï¼‰ã€‚

## 33. é«˜å±‚è¡Œä¸º

1. 34. æ”¶é›†å…¥ç«™é™„ä»¶ï¼ˆ`MediaPaths`ã€`MediaUrls`ã€`MediaTypes`ï¼‰ã€‚
2. 35. å¯¹äºæ¯ä¸ªå¯ç”¨çš„èƒ½åŠ›ï¼ˆå›¾åƒ/éŸ³é¢‘/è§†é¢‘ï¼‰ï¼ŒæŒ‰ç­–ç•¥é€‰æ‹©é™„ä»¶ï¼ˆé»˜è®¤ï¼š**ç¬¬ä¸€ä¸ª**ï¼‰ã€‚
3. 36. é€‰æ‹©ç¬¬ä¸€ä¸ªç¬¦åˆæ¡ä»¶çš„æ¨¡å‹æ¡ç›®ï¼ˆå¤§å° + èƒ½åŠ› + é‰´æƒï¼‰ã€‚
4. 37. å¦‚æœæ¨¡å‹å¤±è´¥æˆ–åª’ä½“è¿‡å¤§ï¼Œ**å›é€€åˆ°ä¸‹ä¸€ä¸ªæ¡ç›®**ã€‚
5. 38. æˆåŠŸæ—¶ï¼š
   - 39. `Body` å˜ä¸º `[Image]`ã€`[Audio]` æˆ– `[Video]` å—ã€‚
   - 40. éŸ³é¢‘è®¾ç½® `{{Transcript}}`ï¼›å‘½ä»¤è§£æåœ¨å­˜åœ¨å­—å¹•æ—¶ä½¿ç”¨å­—å¹•æ–‡æœ¬ï¼Œå¦åˆ™ä½¿ç”¨è½¬å†™æ–‡æœ¬ã€‚
   - 41. å­—å¹•ä¼šä½œä¸º `User text:` ä¿ç•™åœ¨å—å†…ã€‚

42) å¦‚æœç†è§£å¤±è´¥æˆ–è¢«ç¦ç”¨ï¼Œ**å›å¤æµç¨‹å°†ç»§ç»­**ï¼Œå¹¶ä½¿ç”¨åŸå§‹æ­£æ–‡ + é™„ä»¶ã€‚

## 43. é…ç½®æ¦‚è§ˆ

44. `tools.media` æ”¯æŒ **å…±äº«æ¨¡å‹** ä»¥åŠæŒ‰èƒ½åŠ›çš„è¦†ç›–ï¼š

- 45. `tools.media.models`ï¼šå…±äº«æ¨¡å‹åˆ—è¡¨ï¼ˆä½¿ç”¨ `capabilities` è¿›è¡Œé—¨æ§ï¼‰ã€‚
- 46. `tools.media.image` / `tools.media.audio` / `tools.media.video`ï¼š
  - 47. é»˜è®¤é¡¹ï¼ˆ`prompt`ã€`maxChars`ã€`maxBytes`ã€`timeoutSeconds`ã€`language`ï¼‰
  - 48. æä¾›å•†è¦†ç›–ï¼ˆ`baseUrl`ã€`headers`ã€`providerOptions`ï¼‰
  - 49. é€šè¿‡ `tools.media.audio.providerOptions.deepgram` çš„ Deepgram éŸ³é¢‘é€‰é¡¹
  - 50. å¯é€‰çš„ **æŒ‰èƒ½åŠ› `models` åˆ—è¡¨**ï¼ˆä¼˜å…ˆäºå…±äº«æ¨¡å‹ï¼‰
  - `attachments` ç­–ç•¥ï¼ˆ`mode`ã€`maxAttachments`ã€`prefer`ï¼‰
  - `scope`ï¼ˆæŒ‰ channel/chatType/session key çš„å¯é€‰é—¨æ§ï¼‰
- `tools.media.concurrency`ï¼šæœ€å¤§å¹¶å‘èƒ½åŠ›è¿è¡Œæ•°ï¼ˆé»˜è®¤ **2**ï¼‰ã€‚

```json5
{
  tools: {
    media: {
      models: [
        /* å…±äº«åˆ—è¡¨ */
      ],
      image: {
        /* å¯é€‰è¦†ç›– */
      },
      audio: {
        /* å¯é€‰è¦†ç›– */
      },
      video: {
        /* å¯é€‰è¦†ç›– */
      },
    },
  },
}
```

### æ¨¡å‹æ¡ç›®

æ¯ä¸ª `models[]` æ¡ç›®å¯ä»¥æ˜¯ **provider** æˆ– **CLI**ï¼š

```json5
{
  type: "provider", // è‹¥çœç•¥åˆ™ä¸ºé»˜è®¤
  provider: "openai",
  model: "gpt-5.2",
  prompt: "Describe the image in <= 500 chars.",
  maxChars: 500,
  maxBytes: 10485760,
  timeoutSeconds: 60,
  capabilities: ["image"], // å¯é€‰ï¼Œç”¨äºå¤šæ¨¡æ€æ¡ç›®
  profile: "vision-profile",
  preferredProfile: "vision-fallback",
}
```

```json5
{
  type: "cli",
  command: "gemini",
  args: [
    "-m",
    "gemini-3-flash",
    "--allowed-tools",
    "read_file",
    "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
  ],
  maxChars: 500,
  maxBytes: 52428800,
  timeoutSeconds: 120,
  capabilities: ["video", "image"],
}
```

CLI æ¨¡æ¿è¿˜å¯ä»¥ä½¿ç”¨ï¼š

- `{{MediaDir}}`ï¼ˆåŒ…å«åª’ä½“æ–‡ä»¶çš„ç›®å½•ï¼‰
- `{{OutputDir}}`ï¼ˆä¸ºæœ¬æ¬¡è¿è¡Œåˆ›å»ºçš„ä¸´æ—¶ç›®å½•ï¼‰
- `{{OutputBase}}`ï¼ˆä¸´æ—¶æ–‡ä»¶åŸºè·¯å¾„ï¼Œæ— æ‰©å±•åï¼‰

## é»˜è®¤å€¼ä¸é™åˆ¶

æ¨èé»˜è®¤å€¼ï¼š

- `maxChars`ï¼šç”¨äºå›¾ç‰‡/è§†é¢‘ä¸º **500**ï¼ˆç®€çŸ­ã€ä¾¿äºå‘½ä»¤è¡Œï¼‰
- `maxChars`ï¼šç”¨äºéŸ³é¢‘ä¸º **æœªè®¾ç½®**ï¼ˆå®Œæ•´è½¬å†™ï¼Œé™¤éä½ è®¾ç½®é™åˆ¶ï¼‰
- `maxBytes`ï¼š
  - å›¾ç‰‡ï¼š**10MB**
  - éŸ³é¢‘ï¼š**20MB**
  - è§†é¢‘ï¼š**50MB**

è§„åˆ™ï¼š

- å¦‚æœåª’ä½“è¶…è¿‡ `maxBytes`ï¼Œåˆ™è·³è¿‡è¯¥æ¨¡å‹å¹¶**å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹**ã€‚
- å¦‚æœæ¨¡å‹è¿”å›å†…å®¹è¶…è¿‡ `maxChars`ï¼Œè¾“å‡ºå°†è¢«æˆªæ–­ã€‚
- `prompt` é»˜è®¤æ˜¯ç®€å•çš„â€œDescribe the {media}.â€ï¼Œå¹¶é™„åŠ  `maxChars` æŒ‡å¼•ï¼ˆä»…å›¾ç‰‡/è§†é¢‘ï¼‰ã€‚
- å¦‚æœ `<capability>.enabled: true` ä½†æœªé…ç½®ä»»ä½•æ¨¡å‹ï¼Œå½“å…¶æä¾›æ–¹æ”¯æŒè¯¥èƒ½åŠ›æ—¶ï¼ŒOpenClaw ä¼šå°è¯•**å½“å‰å›å¤æ¨¡å‹**ã€‚

### è‡ªåŠ¨æ£€æµ‹åª’ä½“ç†è§£ï¼ˆé»˜è®¤ï¼‰

å¦‚æœæœªå°† `tools.media.<capability>
.enabled` è®¾ç½®ä¸º `false`ï¼Œä¸”ä½ å°šæœªé…ç½®æ¨¡å‹ï¼ŒOpenClaw ä¼šæŒ‰ä»¥ä¸‹é¡ºåºè‡ªåŠ¨æ£€æµ‹ï¼Œå¹¶åœ¨**ç¬¬ä¸€ä¸ªå¯ç”¨é€‰é¡¹**å¤„åœæ­¢ï¼š**æœ¬åœ° CLI**ï¼ˆä»…éŸ³é¢‘ï¼›è‹¥å·²å®‰è£…ï¼‰

1. `sherpa-onnx-offline`ï¼ˆéœ€è¦ `SHERPA_ONNX_MODEL_DIR`ï¼ŒåŒ…å« encoder/decoder/joiner/tokensï¼‰
   - `whisper-cli`ï¼ˆ`whisper-cpp`ï¼›ä½¿ç”¨ `WHISPER_CPP_MODEL` æˆ–å†…ç½®çš„ tiny æ¨¡å‹ï¼‰
   - `whisper`ï¼ˆPython CLIï¼›è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼‰
   - **Gemini CLI**ï¼ˆ`gemini`ï¼‰ï¼Œä½¿ç”¨ `read_many_files`
2. **æä¾›æ–¹å¯†é’¥**
3. éŸ³é¢‘ï¼šOpenAI â†’ Groq â†’ Deepgram â†’ Google
   - å›¾ç‰‡ï¼šOpenAI â†’ Anthropic â†’ Google â†’ MiniMax
   - è§†é¢‘ï¼šGoogle
   - è¦ç¦ç”¨è‡ªåŠ¨æ£€æµ‹ï¼Œè¯·è®¾ç½®ï¼š

{
tools: {
media: {
audio: {
enabled: false,
},
},
},
}

```json5
æ³¨æ„ï¼šäºŒè¿›åˆ¶æ£€æµ‹åœ¨ macOS/Linux/Windows ä¸Šå°½åŠ›è€Œä¸ºï¼›è¯·ç¡®ä¿ CLI ä½äº `PATH` ä¸­ï¼ˆæˆ‘ä»¬ä¼šå±•å¼€ `~`ï¼‰ï¼Œæˆ–ä½¿ç”¨å¸¦å®Œæ•´å‘½ä»¤è·¯å¾„çš„æ˜¾å¼ CLI æ¨¡å‹ã€‚
```

èƒ½åŠ›ï¼ˆå¯é€‰ï¼‰

## å¦‚æœä½ è®¾ç½®äº† `capabilities`ï¼Œè¯¥æ¡ç›®ä»…ä¼šé’ˆå¯¹è¿™äº›åª’ä½“ç±»å‹è¿è¡Œã€‚

å¯¹äºå…±äº«åˆ—è¡¨ï¼ŒOpenClaw å¯ä»¥æ¨æ–­é»˜è®¤å€¼ï¼š `openai`ã€`anthropic`ã€`minimax`ï¼š**image**

- `google`ï¼ˆGemini APIï¼‰ï¼š**image + audio + video**
- `groq`ï¼š**audio**
- `deepgram`ï¼š**audio**
- å¯¹äº CLI æ¡ç›®ï¼Œ**è¯·æ˜¾å¼è®¾ç½® `capabilities`** ä»¥é¿å…æ„å¤–åŒ¹é…ã€‚

å¦‚æœçœç•¥ `capabilities`ï¼Œè¯¥æ¡ç›®å°†é€‚ç”¨äºå…¶æ‰€åœ¨çš„åˆ—è¡¨ã€‚
æä¾›æ–¹æ”¯æŒçŸ©é˜µï¼ˆOpenClaw é›†æˆï¼‰

## Provider support matrix (OpenClaw integrations)

| Capability | Provider integration                             | Notes                                                                                |
| ---------- | ------------------------------------------------ | ------------------------------------------------------------------------------------ |
| Image      | OpenAI / Anthropic / Google / others via `pi-ai` | Any image-capable model in the registry works.                       |
| Audio      | OpenAI, Groq, Deepgram, Google                   | Provider transcription (Whisper/Deepgram/Gemini). |
| Video      | Google (Gemini API)           | Provider video understanding.                                        |

## Recommended providers

**Image**

- Prefer your active model if it supports images.
- Good defaults: `openai/gpt-5.2`, `anthropic/claude-opus-4-6`, `google/gemini-3-pro-preview`.

**Audio**

- `openai/gpt-4o-mini-transcribe`, `groq/whisper-large-v3-turbo`, or `deepgram/nova-3`.
- CLI fallback: `whisper-cli` (whisper-cpp) or `whisper`.
- Deepgram setup: [Deepgram (audio transcription)](/providers/deepgram).

**Video**

- `google/gemini-3-flash-preview` (fast), `google/gemini-3-pro-preview` (richer).
- CLI fallback: `gemini` CLI (supports `read_file` on video/audio).

## Attachment policy

Perâ€‘capability `attachments` controls which attachments are processed:

- `mode`: `first` (default) or `all`
- `maxAttachments`ï¼šé™åˆ¶å¤„ç†çš„æ•°é‡ï¼ˆé»˜è®¤ **1**ï¼‰ã€‚
- `prefer`: `first`, `last`, `path`, `url`

When `mode: "all"`, outputs are labeled `[Image 1/2]`, `[Audio 2/2]`, etc.

## Config examples

### 1. Shared models list + overrides

```json5
{
  tools: {
    media: {
      models: [
        { provider: "openai", model: "gpt-5.2", capabilities: ["image"] },
        {
          provider: "google",
          model: "gemini-3-flash-preview",
          capabilities: ["image", "audio", "video"],
        },
        {
          type: "cli",
          command: "gemini",
          args: [
            "-m",
            "gemini-3-flash",
            "--allowed-tools",
            "read_file",
            "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
          ],
          capabilities: ["image", "video"],
        },
      ],
      audio: {
        attachments: { mode: "all", maxAttachments: 2 },
      },
      video: {
        maxChars: 500,
      },
    },
  },
}
```

### 2. Audio + Video only (image off)

```json5
{
  tools: {
    media: {
      audio: {
        enabled: true,
        models: [
          { provider: "openai", model: "gpt-4o-mini-transcribe" },
          {
            type: "cli",
            command: "whisper",
            args: ["--model", "base", "{{MediaPath}}"],
          },
        ],
      },
      video: {
        enabled: true,
        maxChars: 500,
        models: [
          { provider: "google", model: "gemini-3-flash-preview" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
            ],
          },
        ],
      },
    },
  },
}
```

### 3. Optional image understanding

```json5
{
  tools: {
    media: {
      image: {
        enabled: true,
        maxBytes: 10485760,
        maxChars: 500,
        models: [
          { provider: "openai", model: "gpt-5.2" },
          { provider: "anthropic", model: "claude-opus-4-6" },
          {
            type: "cli",
            command: "gemini",
            args: [
              "-m",
              "gemini-3-flash",
              "--allowed-tools",
              "read_file",
              "Read the media at {{MediaPath}} and describe it in <= {{MaxChars}} characters.",
            ],
          },
        ],
      },
    },
  },
}
```

### 4. Multiâ€‘modal single entry (explicit capabilities)

```json5
{
  tools: {
    media: {
      image: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
      audio: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
      video: {
        models: [
          {
            provider: "google",
            model: "gemini-3-pro-preview",
            capabilities: ["image", "video", "audio"],
          },
        ],
      },
    },
  },
}
```

## Status output

When media understanding runs, `/status` includes a short summary line:

```
ğŸ“ Media: image ok (openai/gpt-5.2) Â· audio skipped (maxBytes)
```

This shows perâ€‘capability outcomes and the chosen provider/model when applicable.

## Notes

- Understanding is **bestâ€‘effort**. Errors do not block replies.
- Attachments are still passed to models even when understanding is disabled.
- Use `scope` to limit where understanding runs (e.g. only DMs).

## Related docs

- [Configuration](/gateway/configuration)
- [Image & Media Support](/nodes/images)
